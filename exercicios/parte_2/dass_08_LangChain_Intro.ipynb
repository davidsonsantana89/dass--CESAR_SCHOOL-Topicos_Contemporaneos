{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsonsantana89/dass--CESAR_SCHOOL-Topicos_Contemporaneos/blob/main/exercicios/parte_2/dass_08_LangChain_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DAVIDSON ALVES SANTOS DE SANTANA\n",
        "\n",
        "dass@cesar.school"
      ],
      "metadata": {
        "id": "iz0VB_jcnw5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vykbd706U0J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0a112e-5f3e-43f4-eb31-046158ec8d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA6hy0dHU0J2"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKjahPhGX73t",
        "outputId": "0951e7e6-d519-480f-a369-0511a1b7a780"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.23)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.37)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.43.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.108)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HT_kOiZHU0J3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nKUBjYreU0J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fa83cf-166e-4b50-e4da-b6f919184850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None} id='run-c0196bf6-07b1-418d-84d0-9c1154af23bb-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Olá\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k3Tabwm5ZB0P",
        "outputId": "8eeffe4c-a79a-4cd5-b83c-6221ef804645"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olá! Como posso ajudar você hoje?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_PGbt6U0J4"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb0CyNEHU0J4"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j14Rlk-iU0J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bff13f1-ec0f-4fe1-d506-f2bf61ac6cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "# Runnable\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R1BZBOnuU0J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f062a74c-0117-4f8b-e0fc-1b7abc2f2303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A inteligência artificial é o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Qual a cor do objeto {obj}? Responda apenas com uma palavra indicando a cor.\")\n",
        "\n",
        "# Runnable\n",
        "prompt = prompt_template.invoke({\"obj\": \"limão\"})\n",
        "print(prompt)\n",
        "\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eY2FjKCdSjk",
        "outputId": "4733422b-d47e-40b4-894c-3962cdca7292"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Qual a cor do objeto limão? Responda apenas com uma palavra indicando a cor')]\n",
            "Amarelo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yed5ITe1U0J5"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vzS4ht8VU0J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8eafc02-af1d-4be5-a546-40fd290f1b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None} id='run-fe57c1ad-2c7b-4272-bf13-49b96c48bd63-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nN7Y8NnsU0J6"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fViNzuuOU0J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce90b33-020c-45ac-d899-b9cb5f95130b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.'), HumanMessage(content='Hello, how are you?')]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1FPBCFGHU0J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591ed574-34dc-41a5-80e7-1f423d876540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dXC_MeU0J6"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bcU7-jOjU0J6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5n-zI8_ZU0J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c2a754-b5e0-40ad-be6d-ed42d5f13db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "output = parser.invoke(response)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY1ArcJLU0J7"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jBWSp2X5U0J7"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9ZImUSqrU0J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7c6241-67bf-418d-c01f-27ad9b7ba0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sEQSgkVJU0J7"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y22lwCpeU0J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b15af9a-a5a1-4b13-db66-7f0a92897db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_TZQfeJU0J7"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXtObxzpU0J7"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dIQR565xU0J7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cfa67db-ad57-433b-cd68-c9ed2a04261b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o jogador de futebol levou um lápis para o jogo?\\n\\nPorque ele queria fazer um gol de letra! ⚽✏️'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Faça uma piada sobre {topico}\")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({\"topico\": \"futebol\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2w_AvHbU0J8"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uLjMC_1WU0J8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f104e20f-a3d8-4e59-92c0-fffce53cfb77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A frase \"Que pena que tudo acabou entre nós dois\" expressa um sentimento negativo. A palavra \"pena\" indica tristeza ou arrependimento em relação ao término da relação.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Classifique o sentimento em positivo, neutro ou negativo do {topico}\")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "# chain.invoke({\"topico\": \"O dia está maravilhoso\"})\n",
        "chain.invoke({\"topico\": \"Que pena que tudo acabou entre nós dois.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcWRldopU0J8"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ngzRaO_7U0J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7502bf2-05fb-4732-bb04-60d42ad94c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui está uma função em Python que calcula a soma de dois números:\n",
            "\n",
            "```python\n",
            "def soma(a, b):\n",
            "    return a + b\n",
            "```\n",
            "\n",
            "Você pode usar essa função da seguinte maneira:\n",
            "\n",
            "```python\n",
            "resultado = soma(5, 3)\n",
            "print(resultado)  # Saída: 8\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um programador que converte descrições de funções em código Python.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "result = chain.invoke({\"texto\": \"uma função que calcule a soma de dois números\"})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3qZ7DnU0J8"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8R5QZTLZU0J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9e44c7-f1e3-4ecd-af77-341327a674f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A polyhedron is a three-dimensional shape that has flat faces, edges, and vertices. The faces are the sides of the polyhedron, the edges are the lines where two faces meet, and the vertices are the points where the edges meet.\n",
            "\n",
            "Polyhedra can be classified into different types, such as:\n",
            "\n",
            "1. **Convex Polyhedra**: All of their faces \"point outward,\" and any line connecting two points inside the polyhedron lies entirely within it.\n",
            "\n",
            "2. **Concave Polyhedra**: Have at least one face that \"points inward,\" meaning there are points that, if connected, can extend outside the polyhedron.\n",
            "\n",
            "The most common examples of polyhedra include the cube (which has 6 square faces) and the pyramid (which can have different shapes for its base).\n",
            "\n",
            "In summary, polyhedra are solids that have many faces, and each face is a flat shape!\n"
          ]
        }
      ],
      "source": [
        "prompt_template_explicacao = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um especialista em conhecimentos gerais que explica tópicos de maneira simplificada para o usuário.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "prompt_template_tradutor = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de textos do português para o inglês.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template_explicacao | llm | parser | prompt_template_tradutor | llm | parser\n",
        "\n",
        "result = chain.invoke({\"texto\": \"um poliedro\"})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47FSQJ0vU0J8"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QiaYHtGNU0J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46a4007-afd1-44e8-8966-da7347eb7b64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A Cesar School é uma instituição de ensino que faz parte do grupo CESAR (Centro de Estudos e Sistemas Avançados do Recife). Localizada em Recife, Pernambuco, a escola é voltada para a formação de profissionais nas áreas de tecnologia, inovação e design. Oferece uma variedade de cursos, incluindo programas de graduação, pós-graduação e cursos livres, com foco em tópicos como desenvolvimento de software, design de interação, inteligência artificial e outras áreas relacionadas à transformação digital.\\n\\nA Cesar School é conhecida por seu enfoque prático e pela conexão com o mercado, buscando preparar os alunos para os desafios do mundo profissional. Além disso, a escola frequentemente colabora com empresas e organizações para promover projetos e iniciativas que incentivem a inovação e o empreendedorismo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 14, 'total_tokens': 166}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f905cf32a9', 'finish_reason': 'stop', 'logprobs': None}, id='run-82855e86-080b-4c29-a7cc-2beb0a50a5b3-0', usage_metadata={'input_tokens': 14, 'output_tokens': 152, 'total_tokens': 166})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "llm.invoke(\"O que é a Cesar School?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}